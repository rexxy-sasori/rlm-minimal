# RLM Minimal

Minimal implementation of RLM (Recursive Language Model) with two execution architectures for secure code execution and LLM interaction.

## ğŸ—ï¸ Two Architectures

RLM supports two distinct architectures for executing generated code during inference:

### Architecture 1: Local Execution (Development)
- **Code runs in the same process** as RLM inference
- **Simple setup** - no additional services required
- **Low latency** - direct execution
- **Use case**: Development, testing, trusted code environments

```python
from rlm.local import RLM_REPL

rlm = RLM_REPL(model="gpt-5")
result = rlm.completion(context, query)
```

### Architecture 2: Remote WASM Execution (Production)
- **Code runs in separate WASM execution plane** via HTTP API
- **Complete isolation** between inference and execution
- **Maximum security** - LLM API keys never exposed to execution environment
- **Use case**: Production deployment, multi-tenant environments, untrusted code

```python
from rlm.remote import RemoteREPLFactory
from rlm.local import RLM_REPL

factory = RemoteREPLFactory(wasm_service_url="http://wasm-service:8000")
rlm = RLM_REPL(model="gpt-5")
result = rlm.completion(context, query)
```

## âœ¨ Features

- **Code Execution**: Execute Python code in sandboxed environment (local or WASM)
- **LLM Integration**: Interact with various LLM models (OpenAI and compatible)
- **Recursive Reasoning**: Multi-step reasoning with configurable depth
- **Logging**: Comprehensive logging with TimescaleDB integration
- **Token Cache Tracking**: Monitor LLM token cache usage and cost savings
- **Production Ready**: Kubernetes deployment with secure isolation
- **Scalable**: Independent scaling of inference and execution planes

## ğŸš€ Quick Start

### Option 1: Local Execution (Quickest)

```bash
# Install dependencies
pip install -r requirements.txt

# Run example
python examples/basic_usage.py
```

### Option 2: Remote WASM Execution (Production)

```bash
# Terminal 1: Start WASM service
python -m rlm.wasm.repl_wasm_service --host 0.0.0.0 --port 8000

# Terminal 2: Use RLM with remote execution
python -c "
from rlm.remote import RemoteREPLFactory
from rlm.local import RLM_REPL

factory = RemoteREPLFactory(wasm_service_url='http://localhost:8000')
rlm = RLM_REPL(model='gpt-5')
result = rlm.completion('context', 'What is 42 + 10?')
print(result)
"
```

### Option 3: Sidecar Architecture (Recommended for Production)

```bash
# Build Docker images
docker build -t rlm-inference:latest -f deploy/docker/Dockerfile.rlm-sidecar .
docker build -t wasm-manager:latest -f deploy/docker/Dockerfile.wasm-manager .

# Deploy to Kubernetes
kubectl apply -f deploy/k8s/rlm-sidecar-deployment.yaml

# Test state persistence
python examples/sidecar_state_persistence.py
```

### Option 4: Kubernetes Deployment (Legacy)

See [DEPLOYMENT_GUIDE.md](deploy/docs/DEPLOYMENT_GUIDE.md) for complete instructions.

```bash
# Deploy to k8s
kubectl apply -f deploy/k8s/wasm-repl-deployment.yaml
kubectl apply -f deploy/k8s/rlm-deployment.yaml
kubectl apply -f deploy/k8s/network-policies.yaml
```

## ğŸ“š Documentation

### Architecture Guides (3 Architectures)
- **[Architecture Guide](rlm/ARCHITECTURE_GUIDE.md)**: Complete guide to all three architectures with comparison tables
  - **Architecture 1**: Local Execution (same process)
  - **Architecture 2**: Same-Pod Execution (sidecar pattern)
  - **Architecture 3**: Different-Pod Execution (remote)
- **[Sidecar Architecture Guide](deploy/docs/SIDECAR_ARCHITECTURE_GUIDE.md)**: Detailed Architecture 2 guide with deployment instructions
- **[Secure WASM Architecture Summary](deploy/docs/SECURE_WASM_ARCHITECTURE_SUMMARY.md)**: Architecture 3 quick deployment guide
- **[Secure Architecture](k8s/doc/SECURE_ARCHITECTURE.md)**: Architecture 3 security details

### Deployment Guides
- **[Deployment Guide](deploy/docs/DEPLOYMENT_GUIDE.md)**: Complete production deployment guide
- **[WASM Quick Start](deploy/docs/WASM_QUICKSTART.md)**: WASM execution quick start
- **[WASM REPL Setup](k8s/doc/WASM_REPL_SETUP.md)**: Complete WASM REPL setup guide

### Benchmark Documentation
- **[Benchmark Analysis](benchmarks/docs/BENCHMARK_ANALYSIS.md)**: Analysis of RLM benchmarks (OOLONG, RULER, Deep Research)
- **[Kubernetes Deployment](benchmarks/docs/KUBERNETES_DEPLOYMENT.md)**: Deploy benchmarks to Kubernetes
- **[Dataset Setup](benchmarks/docs/DATASET_SETUP.md)**: Setup benchmark datasets

### Core Documentation
- **[Depth Implementation](doc/DEPTH_IMPLEMENTATION.md)**: Depth parameter implementation details
- **[Query Example](doc/QUERY_EXAMPLE.md)**: Query API usage examples
- **[Dependencies](doc/DEPENDENCIES.md)**: Project dependencies documentation
- **[Change Summary](doc/CHANGE_SUMMARY.md)**: Kubernetes deployment changes summary

### Logging Documentation
- **[Logger README](rlm/logger/README.md)**: Main logger documentation
- **[TimescaleDB Quick Start](rlm/logger/doc/QUICKSTART_TIMESCALE.md)**: Get started with TimescaleDB logging
- **[Token Cache Tracking](rlm/logger/doc/TOKEN_CACHE_TRACKING.md)**: Track token cache usage and cost savings
- **[Query API](rlm/logger/doc/QUERY_API.md)**: Query logged data
- **[Recursive Logging](rlm/logger/doc/RECURSIVE_LOGGING.md)**: Recursive logging implementation

## ğŸ“ Project Structure

```
rlm-minimal/
â”œâ”€â”€ rlm/                          # Main package
â”‚   â”œâ”€â”€ local/                    # Architecture 1: Local Execution
â”‚   â”‚   â”œâ”€â”€ repl.py               # Local REPL environment
â”‚   â”‚   â”œâ”€â”€ rlm_repl.py           # RLM with local execution
â”‚   â”‚   â””â”€â”€ rlm_repl_tsdb.py      # RLM with TimescaleDB
â”‚   â”œâ”€â”€ remote/                   # Architecture 2: Remote Execution
â”‚   â”‚   â”œâ”€â”€ repl_remote.py        # Remote REPL client
â”‚   â”‚   â””â”€â”€ rlm_service.py        # RLM HTTP service
â”‚   â”œâ”€â”€ wasm/                     # WASM Execution Engine
â”‚   â”‚   â”œâ”€â”€ repl_wasm.py          # WASM executor (Pyodide)
â”‚   â”‚   â””â”€â”€ repl_wasm_service.py  # WASM HTTP service
â”‚   â”œâ”€â”€ utils/                    # Shared utilities
â”‚   â”œâ”€â”€ logger/                   # Logging components
â”‚   â”œâ”€â”€ rlm.py                    # Base RLM class
â”‚   â”œâ”€â”€ __init__.py               # Package exports
â”‚   â”œâ”€â”€ ARCHITECTURE_GUIDE.md     # Architecture guide
â”‚   â””â”€â”€ STRUCTURE.txt             # Structure diagram
â”‚
â”œâ”€â”€ deploy/                       # Deployment files
â”‚   â”œâ”€â”€ docker/                   # Docker images
â”‚   â”‚   â”œâ”€â”€ Dockerfile.rlm        # RLM inference image
â”‚   â”‚   â””â”€â”€ Dockerfile.wasm       # WASM execution image
â”‚   â”œâ”€â”€ k8s/                      # Kubernetes config
â”‚   â”‚   â”œâ”€â”€ rlm-deployment.yaml           # RLM deployment
â”‚   â”‚   â”œâ”€â”€ wasm-repl-deployment.yaml     # WASM deployment
â”‚   â”‚   â””â”€â”€ network-policies.yaml         # Network security
â”‚   â””â”€â”€ docs/                     # Deployment docs
â”‚
â”œâ”€â”€ examples/                     # Example scripts
â”‚   â”œâ”€â”€ basic_usage.py            # Basic usage example
â”‚   â””â”€â”€ model_comparison.py       # Model comparison example
â”‚
â”œâ”€â”€ tests/                        # Test suite
â”‚   â””â”€â”€ test_wasm_repl.py         # WASM execution tests
â”‚
â”œâ”€â”€ README.md                     # This file
â”œâ”€â”€ ORGANIZATION_SUMMARY.md       # Reorganization summary
â”œâ”€â”€ requirements.txt              # Core dependencies
â””â”€â”€ requirements-wasm.txt         # WASM-specific dependencies
```

## ğŸ› ï¸ Installation

### Core Dependencies

```bash
pip install -r requirements.txt
```

### WASM Dependencies (for remote execution)

```bash
pip install -r requirements-wasm.txt
```

## ğŸ”§ Configuration

### Environment Variables

```bash
# LLM Configuration
export LLM_API_KEY="your-api-key"
export LLM_BASE_URL="https://api.openai.com/v1"
export LLM_MODEL="gpt-5"

# WASM Service (for remote execution)
export WASM_SERVICE_URL="http://wasm-repl-service:8000"

# Recursion Configuration
export MAX_DEPTH="3"
export MAX_ITERATIONS="20"
```

## ğŸ“– Usage Examples

### Basic Local Execution

```python
from rlm.local import RLM_REPL

# Initialize RLM
rlm = RLM_REPL(
    api_key="your-key",
    model="gpt-5",
    max_depth=3
)

# Run inference
context = "You are a helpful assistant that can execute Python code."
query = "What is the square root of 256?"

result = rlm.completion(context, query)
print(result)
```

### Remote WASM Execution

```python
from rlm.remote import RemoteREPLFactory
from rlm.local import RLM_REPL

# Initialize remote REPL factory
factory = RemoteREPLFactory(
    wasm_service_url="http://wasm-repl-service:8000"
)

# Check if WASM service is healthy
if not factory.health_check():
    print("WASM service not available")

# Use RLM with remote execution
rlm = RLM_REPL(
    api_key="your-key",
    model="gpt-5",
    max_depth=3
)

# Run inference
context = "You are a helpful assistant that can execute Python code."
query = "Calculate 42 * 10 + 58"

result = rlm.completion(context, query)
print(result)
```

### With Logging

```python
from rlm.local import RLM_REPL
from rlm.logger.repl_logger import REPLEnvLogger

# Initialize with logging
rlm = RLM_REPL(
    api_key="your-key",
    model="gpt-5",
    enable_logging=True
)

# Run inference with logging
result = rlm.completion(context, query)
print(result)
```

## ğŸ§ª Testing

```bash
# Run all tests
python -m pytest tests/ -v

# Run specific test
python tests/test_wasm_repl.py

# Run with coverage
python -m pytest tests/ --cov=rlm
```

## ğŸ”’ Security

### Local Execution
- Only execute code you trust
- Never expose to untrusted inputs
- Consider as development-only

### Remote Execution
- Network policies restrict traffic flow
- LLM API keys never reach execution plane
- WASM sandbox provides additional isolation
- Resource limits prevent exhaustion attacks

## ğŸ“Š Performance

### Local Execution
- Low latency (same process)
- No network overhead
- Resource contention possible

### Remote Execution
- Network latency (HTTP communication)
- Connection pooling recommended
- Independent scaling of components
- Deploy in same AZ for best performance

## ğŸ¤ Contributing

Contributions are welcome! Please:

1. Fork the repository
2. Create a feature branch
3. Add tests for new features
4. Ensure all tests pass
5. Submit a pull request

## ğŸ“ License

MIT License - see [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- Pyodide for WASM Python execution
- OpenAI for LLM API
- Kubernetes for orchestration
- TimescaleDB for time-series data

## ğŸ“ Support

For issues or questions:
1. Check the [Architecture Guide](rlm/ARCHITECTURE_GUIDE.md)
2. Review the [Deployment Guide](deploy/docs/DEPLOYMENT_GUIDE.md)
3. Examine the [examples](examples/)
4. Run the [test suite](tests/)
5. Create an issue on GitHub

## ğŸš€ What's Next

1. **Choose Architecture**
   - Local for development
   - Remote WASM for production

2. **Read Documentation**
   - Start with [Architecture Guide](rlm/ARCHITECTURE_GUIDE.md)
   - Then [Deployment Guide](deploy/docs/DEPLOYMENT_GUIDE.md)

3. **Start Coding**
   - Use `from rlm.local import ...` for local execution
   - Use `from rlm.remote import ...` for remote execution

4. **Deploy (if production)**
   - Follow deployment guide
   - Use k8s for orchestration
   - Enable network policies

**Happy coding! ğŸ‰**
