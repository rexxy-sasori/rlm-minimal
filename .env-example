# LLM Configuration
# This file contains configuration options for different LLM backends.
# Rename this file to ".env" to use these settings.

# =============================================================================
# API Key Configuration
# =============================================================================
# For OpenAI-hosted services (default):
OPENAI_API_KEY=your_openai_api_key_here

# For other OpenAI-compatible backends (alternative):
# LLM_API_KEY=your_api_key_here

# =============================================================================
# Backend Configuration
# =============================================================================
# LLM_BASE_URL - Primary configuration for switching between LLM backends
# Uncomment the appropriate line for your desired backend:

# OpenAI (default - no need to set LLM_BASE_URL)
# LLM_BASE_URL=https://api.openai.com/v1

# OpenRouter (multiple models from different providers)
# LLM_BASE_URL=https://openrouter.ai/api/v1

# Local LLM backends (OpenAI-compatible):

# LM Studio (local)
# LLM_BASE_URL=http://localhost:1234/v1

# Ollama (local)
# LLM_BASE_URL=http://localhost:11434/v1

# vLLM (local or self-hosted)
# LLM_BASE_URL=http://localhost:8000/v1

# Text Generation WebUI (with OpenAI extension)
# LLM_BASE_URL=http://localhost:5000/v1

# Self-hosted OpenAI-compatible services
# LLM_BASE_URL=https://your-custom-domain.com/v1

# =============================================================================
# Model Configuration
# =============================================================================
# LLM_MODEL - Specify the model to use for the root RLM (optional)
# If not set, defaults to "gpt-5"
# Examples:
# LLM_MODEL=gpt-5
# LLM_MODEL=gpt-4o
# LLM_MODEL=claude-3-5-sonnet
# LLM_MODEL=llama-3.1-70b-instruct
# LLM_MODEL=mistral-large-2

# LLM_RECURSIVE_MODEL - Specify the model for recursive/sub-calls (optional)
# If not set, defaults to "gpt-5-mini"
# Examples:
# LLM_RECURSIVE_MODEL=gpt-5-mini
# LLM_RECURSIVE_MODEL=gpt-4o-mini
# LLM_RECURSIVE_MODEL=claude-3-opus
# LLM_RECURSIVE_MODEL=llama-3.1-8b-instruct
# LLM_RECURSIVE_MODEL=mistral-small

# LLM_RECURSIVE_BASE_URL - Specify a different backend for recursive/sub-calls (optional)
# Use this if you want the root RLM and recursive calls to use different backends
# Examples:
# LLM_RECURSIVE_BASE_URL=http://localhost:1234/v1
# LLM_RECURSIVE_BASE_URL=https://openrouter.ai/api/v1

# =============================================================================
# Quick Start Examples
# =============================================================================

# Example 1: Using OpenAI (default)
# OPENAI_API_KEY=sk-xxxxxxxxxxxxxxxxxx
# (No LLM_BASE_URL needed)

# Example 2: Using LM Studio locally
# LLM_BASE_URL=http://localhost:1234/v1
# LLM_API_KEY=lm-studio
# LLM_MODEL=your-local-model-name

# Example 3: Using Ollama locally
# LLM_BASE_URL=http://localhost:11434/v1
# LLM_API_KEY=ollama
# LLM_MODEL=llama3.1

# Example 4: Using OpenRouter
# LLM_BASE_URL=https://openrouter.ai/api/v1
# LLM_API_KEY=sk-or-v1-xxxxxxxxxxxxxxxxxx
# LLM_MODEL=anthropic/claude-3-5-sonnet

# Example 5: Hybrid setup - Cloud root model with local recursive model
# Root RLM uses OpenAI GPT-4o for orchestration
# OPENAI_API_KEY=sk-xxxxxxxxxxxxxxxxxx
# LLM_MODEL=gpt-4o
# 
# Recursive calls use local LM Studio for context processing
# LLM_RECURSIVE_BASE_URL=http://localhost:1234/v1
# (recursive_model can be set in code or defaults to "gpt-5-mini")

# Example 6: Different models on same backend (all config via env vars)
# Both root and recursive use OpenRouter but different models
# LLM_BASE_URL=https://openrouter.ai/api/v1
# LLM_API_KEY=sk-or-v1-xxxxxxxxxxxxxxxxxx
# LLM_MODEL=gpt-4o
# LLM_RECURSIVE_MODEL=claude-3-5-sonnet

# Example 7: Full configuration via environment variables only
# No code changes needed - configure everything via .env file
# OPENAI_API_KEY=sk-xxxxxxxxxxxxxxxxxx
# LLM_MODEL=gpt-4o
# LLM_BASE_URL=https://api.openai.com/v1
# LLM_RECURSIVE_MODEL=gpt-5-mini
# LLM_RECURSIVE_BASE_URL=http://localhost:1234/v1

# =============================================================================
# Notes
# =============================================================================
# - The system will automatically detect if you're using a local backend
#   (contains "localhost" or "127.0.0.1") and provide appropriate error messages
# - API keys can be set via OPENAI_API_KEY or LLM_API_KEY (LLM_API_KEY takes priority)
# - Models can be set via LLM_MODEL (root) and LLM_RECURSIVE_MODEL (sub-calls)
# - Backends can be set via LLM_BASE_URL (root) and LLM_RECURSIVE_BASE_URL (sub-calls)
# - Explicit parameters in code override environment variables
# - Root and recursive models can use different backends for cost optimization
#   (e.g., cloud for root orchestration, local for context processing)
# - Full configuration can be done via environment variables - no code changes required!
# - All OpenAI-compatible backends should work with this configuration
